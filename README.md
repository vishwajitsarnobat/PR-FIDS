# Poisoning-Resistant Federated Intrusion Detection System

This repository contains the complete implementation of a **Poisoning-Resistant Federated Intrusion Detection System (IDS)**. The project demonstrates how to build a privacy-preserving, collaborative IDS using Federated Learning (FL) and equips it with a robust, adaptive, 3-layer defense system to protect against data poisoning attacks.

## Table of Contents
1. [Project Overview](#1-project-overview)
2. [Key Features](#2-key-features)
3. [The Problem: Poisoning Federated Learning](#3-the-problem-poisoning-federated-learning)
4. [The Solution: An Adaptive 3-Layer Defense](#4-the-solution-an-adaptive-3-layer-defense)
5. [Results: The Effectiveness of the Defense](#5-results-the-effectiveness-of-the-defense)
6. [System Architecture](#6-system-architecture)
7. [Getting Started](#7-getting-started)
    - [Prerequisites](#prerequisites)
    - [Installation](#installation)
    - [Dataset](#dataset)
8. [How to Run Simulations](#8-how-to-run-simulations)
9. [Project Structure](#9-project-structure)

---

## 1. Project Overview

Federated Learning (FL) enables multiple parties to collaboratively train a powerful machine learning model without sharing their raw, private data. This makes it an ideal paradigm for building an Intrusion Detection System (IDS) that can learn from diverse network environments.

However, this decentralized approach is vulnerable to **poisoning attacks**, where malicious clients send corrupted model updates to degrade the performance and reliability of the central global model.

This project implements a complete FL-based IDS from scratch and demonstrates:
- The effectiveness of a standard FL system in an ideal environment.
- The devastating impact of **Label Flipping** and **Backdoor** poisoning attacks.
- The successful implementation of a multi-layered defense system that neutralizes these attacks and secures the FL process.

---

## 2. Key Features

- **Federated IDS:** A `CNN-LSTM` deep learning model for intrusion detection, trained collaboratively using the Federated Averaging (FedAvg) algorithm.
- **Poisoning Attacks:** Simulation of two potent attack vectors:
    - **Label Flipping:** Malicious clients flip `attack` labels to `normal`.
    - **Backdoor Injection:** Malicious clients inject a hidden trigger to create a vulnerability.
- **Adaptive 3-Layer Defense System:**
    - **Layer 1 (Client Fingerprinting):** Passively monitors the consistency of client updates.
    - **Layer 2 (Adaptive Honeypot):** Actively tests client models against a trusted dataset using a dynamic, relative baseline to detect harmful behavior.
    - **Layer 3 (Robust Aggregation):** Uses the trust scores generated by the other layers to perform a weighted average of model updates, effectively silencing malicious clients.

---

## 3. The Problem: Poisoning Federated Learning

A standard FL system is built on trust. Without defenses, it blindly averages all incoming model updates. Our simulation shows that with just 30% malicious clients, the system's performance becomes unstable and unreliable.

![Impact of Poisoning Attacks](report_2_attack_impact.png)
> **Figure 1:** This plot shows the global model's accuracy under attack. Compared to the stable blue **Baseline**, the undefended **Label Flipping (orange)** and **Backdoor (red)** attacks cause extreme performance volatility, rendering the model untrustworthy.

---

## 4. The Solution: An Adaptive 3-Layer Defense

Our solution is a synergistic defense system where each layer addresses a weakness in the others. The key innovation is the **adaptive honeypot**, which uses a moving performance baseline to accurately distinguish malicious updates from normal client data variations. This provides a reliable trust signal to the robust aggregation layer, which then mitigates the attack by down-weighting or ignoring the malicious contributions.

---

## 5. Results: The Effectiveness of the Defense

The final results demonstrate the conclusive success of our system. The adaptive 3-layer defense effectively neutralizes both types of poisoning attacks, stabilizing the training process and restoring the model's accuracy to the optimal baseline level.

![Defense Effectiveness](report_3_defense_effectiveness.png)
> **Figure 2:** The final "hero plot". The dashed orange and red lines represent the undefended attacks. The solid **green (Defended Label Flipping)** and **purple (Defended Backdoor)** lines show our system under the same attacks. They successfully absorb the attack and converge to the same high, stable accuracy as the blue baseline.

---

## 6. System Architecture

The project consists of a server, multiple clients, and a set of modules for handling data, attacks, and defenses.

- **FL Server (`fl_server.py`):** Manages clients and orchestrates the FL process.
- **FL Clients (`fl_client.py`):** Train the local IDS model on their private data.
- **IDS Model (`ids_model.py`):** A `CNN-LSTM` model for attack detection.
- **Poisoning Attacks (`attacks.py`):** Contains the logic for Label Flipping and Backdoor attacks.
- **Defense System (`defenses.py`):** Implements the 3-layer adaptive defense.

---

## 7. Getting Started

### Prerequisites
- Python 3.10+
- [uv](https://github.com/astral-sh/uv) (recommended) or pip for package management

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/vishwajitsarnobat/PR-FIDS.git
   cd PR-FIDS
   ```

2. **Create and activate a virtual environment:**
   ```bash
   # Using uv
   uv venv
   source .venv/bin/activate

   # Or using Python's venv
   python -m venv .venv
   source .venv/bin/activate
   ```
   *(Note: Follow the official documentation to install **uv** if not installed already:  
    [uv Installation Guide](https://docs.astral.sh/uv/getting-started/installation/))*

3. **Install the required packages:**
   ```bash
   # Using uv
   uv sync
   
   # Or using pip
   pip install -r requirements.txt
   ```   
   *(Note: You may need to create a `requirements.txt` file by running `uv pip freeze > requirements.txt` or `pip freeze > requirements.txt`)*

### Dataset

This project uses the **NSL-KDD Dataset**. You must download it manually.
1. Visit the [NSL-KDD Dataset Page on Kaggle](https://www.kaggle.com/datasets/hassan06/nslkdd).
2. Download entire zip and extract to get `KDDTrain+.txt` and `KDDTest+.txt`.
3. Place both files in the root directory of this project.

---

## 8. How to Run Simulations

All simulations are controlled from `main.py`.

1. **Configure the Simulation:**
   Open `main.py` and edit the `scenarios_to_run` list to include the experiments you want to run. The available scenarios are:
   - `"baseline"`
   - `"attack_label_flipping"`
   - `"attack_backdoor"`
   - `"defended_label_flipping"`
   - `"defended_backdoor"`

2. **Run the Simulation:**
   Execute the main script from your terminal:
   ```bash
   # Using uv
   uv run main.py

   # Or using python
   python3 main.py
   ```
   *(Note: Use python instead of python3 on Windows system)*
   This will run the selected scenarios and save the results to `simulation_results.csv`.

3. **Generate Plots:**
   After the simulation is complete, run the analysis script to generate the plots:
   ```bash
   # Using uv
   uv run main.py

   # Or using python
   python3 results_analysis.py
   ```
   This will read `simulation_results.csv` and save the plots as PNG files in the root directory.

---

## 9. Project Structure

```
.
├── main.py                 # Master script to run all scenarios
├── results_analysis.py     # Generates all plots from simulation results
├── fl_server.py            # Implements the FL Server logic
├── fl_client.py            # Implements the FL Client logic
├── ids_model.py            # Defines the CNN-LSTM model
├── defenses.py             # Contains the 3-layer defense system
├── attacks.py              # Contains the poisoning attack logic
├── data_loader.py          # Preprocesses and distributes the dataset
├── KDDTrain+.txt           # (Must be downloaded)
├── KDDTest+.txt            # (Must be downloaded)
└── README.md               # This file
```
---

## 10. Documentation

[Google Docs Link](https://docs.google.com/document/d/1iCbQGVKUGMK30eZ7k4ftov1jacxLfs7PTE505NhOsCA/edit?usp=sharing)
